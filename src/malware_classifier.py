import sys
from math import log
from pyspark import SparkContext
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from operator import add
sc = SparkContext.getOrCreate()


# implement functions here
def tokenize_and_preproc(dat_rdd):
    data = dat_rdd.flatMapValues(lambda x : x.split())
    data = data.filter(lambda x : len(x[1]) == 2 )
    return(data)

def calculate_priors(rdd):

    rdd=rdd.map(lambda x:(x,1)).reduceByKey(add).sortByKey(True)
    data=rdd.take(10)
    dictionary=dict(data)
    sum_values=sum(dictionary.values())
    priors=rdd.map(lambda x: (x[0],round(x[1]/float(sum_values-1),2)))
    return(priors)

def classify(likelihoods, priors, data):
    """
    creates classifications for each document in data
    parameters:
    likelihoods: an rdd of likelihoods for each class for each word
    priors: an rdd of priors for each class
    data: a pair rdd of tokens for each file
    """
    # join likelihoods onto tokens from test data
    data = data.map(lambda x: (x[1], x[0])).join(likelihoods).map(lambda x: x[1])

    # convert to log likelihoods to avoid overflow
    data = data.mapValues(lambda x: [log(i) for i in x])

    # reduce by filename to create likelihood sums
    data = data.reduceByKey(lambda x, y: [i+j for i, j in zip(x, y)])

    # create and broadcast log priors
    log_priors = priors.mapValues(lambda x: log(x)).values().collect()
    log_priors = sc.broadcast(log_priors)

    # add priors to reduced likelihoods
    data = data.mapValues(lambda x: [i+j for i, j in zip(x, log_priors.value)])

    # argmax for class
    classes = data.mapValues(lambda x: x.index(max(x))+1)
    return classes


def test_smoothing(train_dat, test_dat):
	'''The Function being tested should take labeled training data and
        tokenized testing data and number of classes and return an RDD of the training data with a
        new entry added for each unique (label, word) pair to increase the
        count of all vocab by 1 for each label to avoid 0 probabilities
        '''

		
	label_dat = train_dat.flatMap(lambda x : x)
	label_dat = label_dat.filter(lambda x : len(x) != 2) # retaining only the labels
	label_dat = label_dat.distinct() # retaining only the distinct labels
	n = label.count() # taking the count of distincr labels

	word_dat = train_dat.flatMap(lambda x : x)
	word_dat = word_dat.filter(lambda x : len(x) == 2) # retaining only the words
	word_dat = word_dat.distinct() # retaining only the distinct words

	mod_dat = label_dat.cartesian(word_dat) # taking cartesian product of each word in the training data with each class
	train_complete = train_dat.union(mod_dat)
	
	mod_train = train_dat.flatMap(lambda x : x)
	mod_train = mod_train.filter(lambda x : len(x) == 2) # retaining only the hex binaries

	mod_test = test_dat.flatMap(lambda x : x)
	mod_test = mod_test.filter(lambda x : len(x) == 2) # retaining only the hex binaries

	diff = mod_test.subtract(mod_train) # retaining the hex binaries which are present in test but not in training data
	new_rdd = diff.flatMap(lambda x: ((str(i),x)for i in range(1,n+1))) # adding missing hex binaries with each label in an rdd

	complete_train = train_complete.union(new_rdd) # appending the rdd with missing hex binaries to the training data rdd
	return complete_train


# implement command line call w/ arguments here
if __name__ == '__main__':
    """
    This is the driver of the malware_classifier, it should take in
    two files for training with filenames (argv[1]) to pull in from a
    directory of malware byte files, and the respective labels
    (argv[2]) for each. This should then take a directory of byte
    files (argv[3]) to read in based on the filenames provided.

    This then repeats the process to read in test data defined by
    the testing filenames (argv[4])

    Once the training data is fully read in and distributed, the
    pipeline for training will call each of the functions defined
    above to train the classifier and apply it to test input
    """

    # read in the training filenames and labels
    X_train = sc.textFile(sys.argv[1])
    y_train = sc.textFile(sys.argv[2])

    # read directory of byte files
    byte_data_directory = sys.argv[3]

    # form full filename from directory and names and loas into list
    X_filenames = X_train.map(lambda x: byte_data_directory+x+'.bytes')
    X_filenames = X_filenames.collect()

    # load pairRDD of text to preproc and train on
    dat_train = sc.wholeTextFiles(",".join(X_filenames))

    # create constant time map of filename to label
    label_map = dict(zip(X_filenames,y_train.collect()))

    # read in the testing data using same process as above
    X_test = sc.textFile(sys.argv[4])
    X_test_filenames = X_test.map(lambda x: byte_data_directory+x+'.bytes')
    X_test_filenames = X_test_filenames.collect()
    dat_test = sc.wholeTextFiles(",".join(X_test_filenames))
