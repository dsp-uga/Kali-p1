import sys
from math import log
from pyspark import SparkContext
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from operator import add
sc = SparkContext.getOrCreate()


# implement functions here
def tokenize_and_preproc(dat_rdd):
    data = dat_rdd.flatMapValues(lambda x : x.split())
    data = data.filter(lambda x : len(x[1]) == 2 )
    return(data)

def calculate_priors(rdd):

    rdd=rdd.map(lambda x:(x,1)).reduceByKey(add).sortByKey(True)
    data=rdd.take(10)
    dictionary=dict(data)
    sum_values=sum(dictionary.values())
    priors=rdd.map(lambda x: (x[0],round(x[1]/float(sum_values-1),2)))
    return(priors)

def calculate_likelihood(dat_rdd):
    """
    calculate the likelihood for each token in every class
    parameters:
    dat_rdd: an rdd of pair each class for each word, (word, class)
    """
    
    # total word count for each token
    word_count1 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 1), word[1])).collect()
    word_count2 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 2), word[1])).collect()        
    word_count3 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 3), word[1])).collect()        
    word_count4 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 4), word[1])).collect()        
    word_count5 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 5), word[1])).collect()   
    word_count6 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 5), word[1])).collect()
    word_count7 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 5), word[1])).collect()
    word_count8 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 5), word[1])).collect()
    word_count9 = dat_rdd.map(lambda word: (word[0], 1)).reduceByKey(lambda a, b: a+b).map(lambda word: ((word[0], 5), word[1])).collect()
    
    # collect all the word count
    word_count = word_count1 + word_count2 + word_count3 + word_count4 + word_count5 + word_count6 + word_count7 + word_count8 + word_count9
    word_count = sc.parallelize(word_count) 
    
    # calculate the (word, class) pairs 
    word_class = sc.parallelize(dat_rdd).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b).sortBy(lambda x: x[1], ascending=True)
    
    # join both the counts
    dvl = word_count.join(word_class).sortBy(lambda x: x[0][1], ascending=True)
    
    # calculate the likelihood: No. of occurance in a single class / No. of occurance in all the classes
    likelihood = dvl.map(lambda a: (a[0][0], a[1][1]/a[1][0])).groupByKey().map(lambda x: (x[0], list(x[1])))
    return(likelihood)

def classify(likelihoods, priors, data):
    """
    creates classifications for each document in data
    parameters:
    likelihoods: an rdd of likelihoods for each class for each word
    priors: an rdd of priors for each class
    data: a pair rdd of tokens for each file
    """
    # join likelihoods onto tokens from test data
    data = data.map(lambda x: (x[1], x[0])).join(likelihoods).map(lambda x: x[1])

    # convert to log likelihoods to avoid overflow
    data = data.mapValues(lambda x: [log(i) for i in x])

    # reduce by filename to create likelihood sums
    data = data.reduceByKey(lambda x, y: [i+j for i, j in zip(x, y)])

    # create and broadcast log priors
    log_priors = priors.mapValues(lambda x: log(x)).values().collect()
    log_priors = sc.broadcast(log_priors)

    # add priors to reduced likelihoods
    data = data.mapValues(lambda x: [i+j for i, j in zip(x, log_priors.value)])

    # argmax for class
    classes = data.mapValues(lambda x: x.index(max(x))+1)
    return classes

# implement command line call w/ arguments here
if __name__ == '__main__':
    """
    This is the driver of the malware_classifier, it should take in
    two files for training with filenames (argv[1]) to pull in from a
    directory of malware byte files, and the respective labels
    (argv[2]) for each. This should then take a directory of byte
    files (argv[3]) to read in based on the filenames provided.

    This then repeats the process to read in test data defined by
    the testing filenames (argv[4])

    Once the training data is fully read in and distributed, the
    pipeline for training will call each of the functions defined
    above to train the classifier and apply it to test input
    """

    # read in the training filenames and labels
    X_train = sc.textFile(sys.argv[1])
    y_train = sc.textFile(sys.argv[2])

    # read directory of byte files
    byte_data_directory = sys.argv[3]

    # form full filename from directory and names and loas into list
    X_filenames = X_train.map(lambda x: byte_data_directory+x+'.bytes')
    X_filenames = X_filenames.collect()

    # load pairRDD of text to preproc and train on
    dat_train = sc.wholeTextFiles(",".join(X_filenames))

    # create constant time map of filename to label
    label_map = dict(zip(X_filenames,y_train.collect()))

    # read in the testing data using same process as above
    X_test = sc.textFile(sys.argv[4])
    X_test_filenames = X_test.map(lambda x: byte_data_directory+x+'.bytes')
    X_test_filenames = X_test_filenames.collect()
    dat_test = sc.wholeTextFiles(",".join(X_test_filenames))
